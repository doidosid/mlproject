{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: (3000, 2)\n",
      "ë¼ë²¨ ë¶„í¬:\n",
      "0    1000\n",
      "1    1000\n",
      "2    1000\n",
      "Name: label, dtype: int64\n",
      "ê°ì • ë§¤í•‘: {0: 'ìš°ìš¸', 1: 'ë¶ˆì•ˆ', 2: 'ì •ìƒ'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "df = pd.read_csv('../data/processed/korean_sentiment_dataset.csv')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "print(f\"ë¼ë²¨ ë¶„í¬:\\n{df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "emotion_map = {0: 'ìš°ìš¸', 1: 'ë¶ˆì•ˆ', 2: 'ì •ìƒ'}\n",
    "print(\"ê°ì • ë§¤í•‘:\", emotion_map)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨: 2400, í…ŒìŠ¤íŠ¸: 600\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"í›ˆë ¨: {len(X_train)}, í…ŒìŠ¤íŠ¸: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF íŠ¹ì„± ìˆ˜: 1217\n",
      "í›ˆë ¨ ë²¡í„° í¬ê¸°: (2400, 1217)\n",
      "í…ŒìŠ¤íŠ¸ ë²¡í„° í¬ê¸°: (600, 1217)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF íŠ¹ì„± ìˆ˜: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"í›ˆë ¨ ë²¡í„° í¬ê¸°: {X_train_tfidf.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë²¡í„° í¬ê¸°: {X_test_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ì •í™•ë„: 1.0000\n",
      "SVM ì •í™•ë„: 1.0000\n",
      "SVM ì •í™•ë„: 1.0000\n",
      "Random Forest ì •í™•ë„: 1.0000\n",
      "Random Forest ì •í™•ë„: 1.0000\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Logistic Regression' : LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM' : SVC(random_state=42, kernel='linear'),\n",
    "    'Random Forest' : RandomForestClassifier(random_state=42,n_estimators=100)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        'model' : model,\n",
    "        'accuracy' : accuracy,\n",
    "        'predictions' : y_pred,\n",
    "    }\n",
    "    print(f\"{name} ì •í™•ë„: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: Logistic Regression\n",
      "ì •í™•ë„: 1.0000\n",
      "==================================================\n",
      "\n",
      "Logistic Regression ìƒì„¸ ì„±ëŠ¥:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ìš°ìš¸       1.00      1.00      1.00       200\n",
      "          ë¶ˆì•ˆ       1.00      1.00      1.00       200\n",
      "          ì •ìƒ       1.00      1.00      1.00       200\n",
      "\n",
      "    accuracy                           1.00       600\n",
      "   macro avg       1.00      1.00      1.00       600\n",
      "weighted avg       1.00      1.00      1.00       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "best_model = results[best_model_name]['model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"ğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}\")\n",
    "print(f\"ì •í™•ë„: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "print(f\"\\n{best_model_name} ìƒì„¸ ì„±ëŠ¥:\")\n",
    "emotions = ['ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì •ìƒ']\n",
    "print(classification_report(y_test, best_predictions, target_names=emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    try:\n",
    "        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½\n",
    "        model = joblib.load('../models/best_model.pkl')  \n",
    "        tfidf = joblib.load('../models/tfidf_vectorizer.pkl') \n",
    "        return model, tfidf\n",
    "    except Exception as e:\n",
    "        st.error(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def predict_emotion(text):\n",
    "    cleaned = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    text_tfidf = tfidf.transform([cleaned])\n",
    "    prediction = best_model.predict(text_tfidf)[0]\n",
    "    \n",
    "    return emotion_map[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:\n",
      "==================================================\n",
      "ë¬¸ì¥: 'ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•˜ê³  ìŠ¬í¼ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "\n",
      "ë¬¸ì¥: 'ì‹œí—˜ì´ ë„ˆë¬´ ê±±ì •ë˜ê³  ë¶ˆì•ˆí•´ìš”'\n",
      "ì˜ˆì¸¡: ë¶ˆì•ˆ\n",
      "\n",
      "ë¬¸ì¥: 'ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "\n",
      "ë¬¸ì¥: 'ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ê³  ì˜ìš•ì´ ì—†ì–´ìš”'\n",
      "ì˜ˆì¸¡: ìš°ìš¸\n",
      "\n",
      "ë¬¸ì¥: 'í˜¹ì‹œ ì‹¤ìˆ˜í• ê¹Œë´ ê³„ì† ê±±ì •ë¼ìš”'\n",
      "ì˜ˆì¸¡: ë¶ˆì•ˆ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸\n",
    "test_sentences = [\n",
    "    \"ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•˜ê³  ìŠ¬í¼ìš”\",\n",
    "    \"ì‹œí—˜ì´ ë„ˆë¬´ ê±±ì •ë˜ê³  ë¶ˆì•ˆí•´ìš”\", \n",
    "    \"ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”\",\n",
    "    \"ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ê³  ì˜ìš•ì´ ì—†ì–´ìš”\",\n",
    "    \"í˜¹ì‹œ ì‹¤ìˆ˜í• ê¹Œë´ ê³„ì† ê±±ì •ë¼ìš”\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸:\")\n",
    "print(\"=\"*50)\n",
    "for sentence in test_sentences:\n",
    "    emotion = predict_emotion(sentence)\n",
    "    print(f\"ë¬¸ì¥: '{sentence}'\")\n",
    "    print(f\"ì˜ˆì¸¡: {emotion}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ ëª¨ë¸ íŒŒì¼ ì €ì¥ ì„±ê³µ!\n"
     ]
    }
   ],
   "source": [
    "# ì €ì¥ í™•ì¸\n",
    "import os\n",
    "if os.path.exists('../models/best_model.pkl'):\n",
    "    print(\"ğŸ‰ ëª¨ë¸ íŒŒì¼ ì €ì¥ ì„±ê³µ!\")\n",
    "else:\n",
    "    print(\"âŒ ëª¨ë¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ìƒˆë¡œìš´ ëª¨ë¸ ì €ì¥ ì¤‘...\n",
      "==================================================\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
      "ğŸ“Š ëª¨ë¸: Logistic Regression\n",
      "ğŸ¯ ì •í™•ë„: 1.0000\n",
      "ğŸ“ ì €ì¥ ìœ„ì¹˜:\n",
      "  - ../models/best_model.pkl\n",
      "  - ../models/tfidf_vectorizer.pkl\n",
      "\n",
      "ğŸ‰ ì´ì œ ì›¹ì•±ì—ì„œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
      "ğŸ“Š ëª¨ë¸: Logistic Regression\n",
      "ğŸ¯ ì •í™•ë„: 1.0000\n",
      "ğŸ“ ì €ì¥ ìœ„ì¹˜:\n",
      "  - ../models/best_model.pkl\n",
      "  - ../models/tfidf_vectorizer.pkl\n",
      "\n",
      "ğŸ‰ ì´ì œ ì›¹ì•±ì—ì„œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ ìƒˆë¡œìš´ ëª¨ë¸ ì €ì¥\n",
    "print(\"ğŸ’¾ ìƒˆë¡œìš´ ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ëª¨ë¸ê³¼ TF-IDF ë²¡í„°ë¼ì´ì € ì €ì¥\n",
    "joblib.dump(best_model, '../models/best_model.pkl')\n",
    "joblib.dump(tfidf, '../models/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“Š ëª¨ë¸: {best_model_name}\")\n",
    "print(f\"ğŸ¯ ì •í™•ë„: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜:\")\n",
    "print(f\"  - ../models/best_model.pkl\")\n",
    "print(f\"  - ../models/tfidf_vectorizer.pkl\")\n",
    "print(\"\\nğŸ‰ ì´ì œ ì›¹ì•±ì—ì„œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ëª¨ë¸ ì„±ëŠ¥ ì¬í™•ì¸:\n",
      "ìµœê³  ëª¨ë¸: Logistic Regression\n",
      "ì •í™•ë„: 1.0000\n",
      "\n",
      "ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\n",
      "0    200\n",
      "1    200\n",
      "2    200\n",
      "dtype: int64\n",
      "\n",
      "ì‹¤ì œ ë¼ë²¨ ë¶„í¬:\n",
      "0    200\n",
      "1    200\n",
      "2    200\n",
      "Name: label, dtype: int64\n",
      "\n",
      "ì˜ˆì¸¡ ë¹„ìœ¨:\n",
      "ìš°ìš¸: 200ê°œ (33.3%)\n",
      "ë¶ˆì•ˆ: 200ê°œ (33.3%)\n",
      "ì •ìƒ: 200ê°œ (33.3%)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ëª¨ë¸ ì„±ëŠ¥ ì§„ë‹¨\n",
    "print(\"ğŸ” ëª¨ë¸ ì„±ëŠ¥ ì¬í™•ì¸:\")\n",
    "print(f\"ìµœê³  ëª¨ë¸: {best_model_name}\")\n",
    "print(f\"ì •í™•ë„: {results[best_model_name]['accuracy']:.4f}\")\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬ í™•ì¸\n",
    "print(\"\\nì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\")\n",
    "pred_counts = pd.Series(best_predictions).value_counts().sort_index()\n",
    "print(pred_counts)\n",
    "\n",
    "print(\"\\nì‹¤ì œ ë¼ë²¨ ë¶„í¬:\")\n",
    "true_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "print(true_counts)\n",
    "\n",
    "# ì˜ˆì¸¡ ë¹„ìœ¨ í™•ì¸\n",
    "print(\"\\nì˜ˆì¸¡ ë¹„ìœ¨:\")\n",
    "for i in range(3):\n",
    "    emotion = ['ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì •ìƒ'][i]\n",
    "    if i in pred_counts.index:\n",
    "        ratio = pred_counts[i] / len(y_test) * 100\n",
    "        print(f\"{emotion}: {pred_counts[i]}ê°œ ({ratio:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{emotion}: 0ê°œ (0.0%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ì›¹ì•± ì „ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n",
      "ì›ë³¸: 'ìš°ìš¸í•´ìš”'\n",
      "ì •ë¦¬: 'ìš°ìš¸í•´ìš”'\n",
      "ì²˜ë¦¬: 'ìš°ìš¸í•´ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "í™•ë¥ : ['0.322', '0.326', '0.352']\n",
      "------------------------------\n",
      "ì›ë³¸: 'ìš°ìš¸í•˜ë‹¤'\n",
      "ì •ë¦¬: 'ìš°ìš¸í•˜ë‹¤'\n",
      "ì²˜ë¦¬: 'ìš°ìš¸í•˜ë‹¤'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "í™•ë¥ : ['0.322', '0.326', '0.352']\n",
      "------------------------------\n",
      "ì›ë³¸: 'ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•´ìš”'\n",
      "ì •ë¦¬: 'ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•´ìš”'\n",
      "ì²˜ë¦¬: 'ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•´ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "í™•ë¥ : ['0.091', '0.093', '0.816']\n",
      "------------------------------\n",
      "ì›ë³¸: 'ìš°ìš¸í•˜ê³  ìŠ¬í¼ìš”'\n",
      "ì •ë¦¬: 'ìš°ìš¸í•˜ê³  ìŠ¬í¼ìš”'\n",
      "ì²˜ë¦¬: 'ìš°ìš¸í•˜ê³  ìŠ¬í¼ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "í™•ë¥ : ['0.322', '0.326', '0.352']\n",
      "------------------------------\n",
      "ì›ë³¸: 'ë¶ˆì•ˆí•´ìš”'\n",
      "ì •ë¦¬: 'ë¶ˆì•ˆí•´ìš”'\n",
      "ì²˜ë¦¬: 'ë¶ˆì•ˆí•´ìš”'\n",
      "ì˜ˆì¸¡: ë¶ˆì•ˆ\n",
      "í™•ë¥ : ['0.123', '0.730', '0.147']\n",
      "------------------------------\n",
      "ì›ë³¸: 'ê±±ì •ë¼ìš”'\n",
      "ì •ë¦¬: 'ê±±ì •ë¼ìš”'\n",
      "ì²˜ë¦¬: 'ê±±ì •ë¼ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "í™•ë¥ : ['0.322', '0.326', '0.352']\n",
      "------------------------------\n",
      "ì›ë³¸: 'ê¸°ë¶„ì´ ì¢‹ì•„ìš”'\n",
      "ì •ë¦¬: 'ê¸°ë¶„ì´ ì¢‹ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ê¸°ë¶„ì´ ì¢‹ì•„ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "í™•ë¥ : ['0.044', '0.045', '0.911']\n",
      "------------------------------\n",
      "ì›ë³¸: 'í–‰ë³µí•´ìš”'\n",
      "ì •ë¦¬: 'í–‰ë³µí•´ìš”'\n",
      "ì²˜ë¦¬: 'í–‰ë³µí•´ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ\n",
      "í™•ë¥ : ['0.322', '0.326', '0.352']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ì›¹ì•±ê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬ë¡œ ì‹¤ì œ í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ§ª ì›¹ì•± ì „ì²˜ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def clean_text_webapp(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "STOPWORDS = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'í•˜ë‹¤']\n",
    "\n",
    "def remove_stopwords_webapp(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in STOPWORDS and len(word) > 1])\n",
    "\n",
    "def predict_webapp_style(text):\n",
    "    # ì›¹ì•±ê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬\n",
    "    cleaned = clean_text_webapp(text)\n",
    "    processed = remove_stopwords_webapp(cleaned)\n",
    "    \n",
    "    print(f\"ì›ë³¸: '{text}'\")\n",
    "    print(f\"ì •ë¦¬: '{cleaned}'\")\n",
    "    print(f\"ì²˜ë¦¬: '{processed}'\")\n",
    "    \n",
    "    if not processed.strip():\n",
    "        print(\"âš ï¸ ì „ì²˜ë¦¬ í›„ ë¹ˆ ë¬¸ìì—´!\")\n",
    "        return None, None\n",
    "    \n",
    "    # ë²¡í„°í™” ë° ì˜ˆì¸¡\n",
    "    text_tfidf = tfidf.transform([processed])\n",
    "    prediction = best_model.predict(text_tfidf)[0]\n",
    "    probability = best_model.predict_proba(text_tfidf)[0]\n",
    "    \n",
    "    emotion_map = {0: 'ìš°ìš¸', 1: 'ë¶ˆì•ˆ', 2: 'ì •ìƒ'}\n",
    "    \n",
    "    print(f\"ì˜ˆì¸¡: {emotion_map[prediction]}\")\n",
    "    print(f\"í™•ë¥ : {[f'{p:.3f}' for p in probability]}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    return emotion_map[prediction], probability\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
    "test_sentences = [\n",
    "    \"ìš°ìš¸í•´ìš”\",\n",
    "    \"ìš°ìš¸í•˜ë‹¤\",\n",
    "    \"ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•´ìš”\",\n",
    "    \"ìš°ìš¸í•˜ê³  ìŠ¬í¼ìš”\",\n",
    "    \"ë¶ˆì•ˆí•´ìš”\",\n",
    "    \"ê±±ì •ë¼ìš”\",\n",
    "    \"ê¸°ë¶„ì´ ì¢‹ì•„ìš”\",\n",
    "    \"í–‰ë³µí•´ìš”\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "for sentence in test_sentences:\n",
    "    predict_webapp_style(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì›ë³¸ ë°ì´í„° 'ìš°ìš¸' ê´€ë ¨ ë¶„ì„\n",
      "==================================================\n",
      "'ìš°ìš¸' ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜: 0\n",
      "\n",
      "ê° ë¼ë²¨ë³„ 'ìš°ìš¸' í¬í•¨ ë¬¸ì¥:\n",
      "ìš°ìš¸: 0ê°œ\n",
      "\n",
      "ë¶ˆì•ˆ: 0ê°œ\n",
      "\n",
      "ì •ìƒ: 0ê°œ\n",
      "\n",
      "\n",
      "'ë¶ˆì•ˆ' ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜: 319\n",
      "\n",
      "ê° ë¼ë²¨ë³„ 'ë¶ˆì•ˆ' í¬í•¨ ë¬¸ì¥:\n",
      "ìš°ìš¸: 0ê°œ\n",
      "ë¶ˆì•ˆ: 319ê°œ\n",
      "ì •ìƒ: 0ê°œ\n",
      "'ìš°ìš¸' ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜: 0\n",
      "\n",
      "ê° ë¼ë²¨ë³„ 'ìš°ìš¸' í¬í•¨ ë¬¸ì¥:\n",
      "ìš°ìš¸: 0ê°œ\n",
      "\n",
      "ë¶ˆì•ˆ: 0ê°œ\n",
      "\n",
      "ì •ìƒ: 0ê°œ\n",
      "\n",
      "\n",
      "'ë¶ˆì•ˆ' ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜: 319\n",
      "\n",
      "ê° ë¼ë²¨ë³„ 'ë¶ˆì•ˆ' í¬í•¨ ë¬¸ì¥:\n",
      "ìš°ìš¸: 0ê°œ\n",
      "ë¶ˆì•ˆ: 319ê°œ\n",
      "ì •ìƒ: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ì›ë³¸ ë°ì´í„°ì—ì„œ 'ìš°ìš¸' ê´€ë ¨ ë‹¨ì–´ ë¶„ì„\n",
    "print(\"ğŸ“Š ì›ë³¸ ë°ì´í„° 'ìš°ìš¸' ê´€ë ¨ ë¶„ì„\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "raw_df = pd.read_csv('../data/raw/korean_sentiment_dataset.csv')\n",
    "\n",
    "# 'ìš°ìš¸' ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ ì°¾ê¸°\n",
    "depression_texts = raw_df[raw_df['text'].str.contains('ìš°ìš¸', na=False)]\n",
    "print(f\"'ìš°ìš¸' ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜: {len(depression_texts)}\")\n",
    "\n",
    "print(\"\\nê° ë¼ë²¨ë³„ 'ìš°ìš¸' í¬í•¨ ë¬¸ì¥:\")\n",
    "for label in [0, 1, 2]:\n",
    "    emotion = ['ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì •ìƒ'][label]\n",
    "    count = len(depression_texts[depression_texts['label'] == label])\n",
    "    print(f\"{emotion}: {count}ê°œ\")\n",
    "    \n",
    "    # ìƒ˜í”Œ ì¶œë ¥\n",
    "    samples = depression_texts[depression_texts['label'] == label]['text'].head(3).tolist()\n",
    "    for i, text in enumerate(samples, 1):\n",
    "        print(f\"  {i}. {text}\")\n",
    "    print()\n",
    "\n",
    "# 'ë¶ˆì•ˆ' ë‹¨ì–´ë„ ë¹„êµí•´ë³´ì\n",
    "anxiety_texts = raw_df[raw_df['text'].str.contains('ë¶ˆì•ˆ', na=False)]\n",
    "print(f\"\\n'ë¶ˆì•ˆ' ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜: {len(anxiety_texts)}\")\n",
    "\n",
    "print(\"\\nê° ë¼ë²¨ë³„ 'ë¶ˆì•ˆ' í¬í•¨ ë¬¸ì¥:\")\n",
    "for label in [0, 1, 2]:\n",
    "    emotion = ['ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì •ìƒ'][label]\n",
    "    count = len(anxiety_texts[anxiety_texts['label'] == label])\n",
    "    print(f\"{emotion}: {count}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ê° ê°ì •ë³„ ì‹¤ì œ ë¬¸ì¥ ìƒ˜í”Œ\n",
      "============================================================\n",
      "\n",
      "ğŸ­ ìš°ìš¸ ë¼ë²¨ ìƒ˜í”Œ (10ê°œ):\n",
      "   1. ì‚¬ì‹¤ì€ í•˜ë£¨í•˜ë£¨ê°€ ë„ˆë¬´ í˜ë“¤ì–´ìš”ì…ë‹ˆë‹¤.\n",
      "   2. ê°€ë”ì€ ë§ˆìŒì´ ìê¾¸ ê°€ë¼ì•‰ê³  ì–´ë‘ì›Œìš” ë„ˆë¬´ì˜€ì–´ìš”.\n",
      "   3. ì–´ëŠ ìˆœê°„ë¶€í„° ì•„ë¬´ ì´ìœ  ì—†ì´ ëˆˆë¬¼ì´ ë‚˜ìš” ë„ˆë¬´ì˜€ì–´ìš”.\n",
      "   4. ì–´ëŠ ìˆœê°„ë¶€í„° ê¸°ìš´ì´ í•˜ë‚˜ë„ ì—†ê³  ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”ì…ë‹ˆë‹¤.\n",
      "   5. ì´ìƒí•˜ê²Œë„ ëª¨ë“  ê²Œ ë‹¤ ë‚˜ ë•Œë¬¸ì¸ ê²ƒ ê°™ì•„ìš” ê´œíˆí–ˆì–´ìš”.\n",
      "   6. ì´ìƒí•˜ê²Œë„ ì¼ì–´ë‚˜ê¸°ê°€ ë„ˆë¬´ í˜ë“¤ì–´ìš” í•­ìƒìš”.\n",
      "   7. ìš”ì¦˜ ë”°ë¼ ì¼ì–´ë‚˜ê¸°ê°€ ë„ˆë¬´ í˜ë“¤ì–´ìš” ë„ˆë¬´í–ˆì–´ìš”.\n",
      "   8. ê·¸ëƒ¥ ê¸°ìš´ì´ í•˜ë‚˜ë„ ì—†ê³  ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš” ì •ë§í–ˆì–´ìš”.\n",
      "   9. ìš”ì¦˜ ë”°ë¼ ìš”ì¦˜ ì‚¬ëŠ” ê²Œ ë¬´ì˜ë¯¸í•˜ê²Œ ëŠê»´ì ¸ìš” ì •ë§í•˜ë„¤ìš”.\n",
      "  10. ì†”ì§íˆ ìš”ì¦˜ ì‚¬ëŠ” ê²Œ ë¬´ì˜ë¯¸í•˜ê²Œ ëŠê»´ì ¸ìš” ë³„ ì´ìœ  ì—†ì´.\n",
      "\n",
      "ìš°ìš¸ ë¼ë²¨ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤:\n",
      "    ë„ˆë¬´: 299íšŒ\n",
      "    ìš”ì¦˜: 234íšŒ\n",
      "    ì—†ê³ : 197íšŒ\n",
      "    ì´ìœ : 194íšŒ\n",
      "    í˜ë“¤ì–´ìš”: 160íšŒ\n",
      "    ì–´ëŠ: 120íšŒ\n",
      "    ìˆœê°„ë¶€í„°: 120íšŒ\n",
      "    ê·¸ëƒ¥: 118íšŒ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ­ ë¶ˆì•ˆ ë¼ë²¨ ìƒ˜í”Œ (10ê°œ):\n",
      "   1. ê·¸ëƒ¥ ì‚¬ëŒë“¤ ì•ì— ì„œëŠ” ê²Œ ë„ˆë¬´ ë¬´ì„œì›Œìš” ê´œíˆì…ë‹ˆë‹¤.\n",
      "   2. ì†”ì§íˆ ê³„ì† ê°€ìŠ´ì´ ë‹µë‹µí•˜ê³  ì¡°ë§ˆì¡°ë§ˆí•´ìš”ì…ë‹ˆë‹¤.\n",
      "   3. ì‘ì€ ì¼ì—ë„ ê¹œì§ê¹œì§ ë†€ë¼ìš” ì •ë§í–ˆì–´ìš”.\n",
      "   4. ì†”ì§íˆ ì‚¬ëŒë“¤ ì•ì— ì„œëŠ” ê²Œ ë„ˆë¬´ ë¬´ì„œì›Œìš” ìê¾¸.\n",
      "   5. ì–´ëŠ ìˆœê°„ë¶€í„° ë¶ˆì•ˆí•´ì„œ ì ì„ ì˜ ìˆ˜ê°€ ì—†ì–´ìš” í•­ìƒì˜€ì–´ìš”.\n",
      "   6. ì´ìƒí•˜ê²Œë„ ì‘ì€ ì¼ì—ë„ ê¹œì§ê¹œì§ ë†€ë¼ìš” ë„ˆë¬´í•˜ë„¤ìš”.\n",
      "   7. ì–´ëŠ ìˆœê°„ë¶€í„° ì‚¬ëŒë“¤ ì•ì— ì„œëŠ” ê²Œ ë„ˆë¬´ ë¬´ì„œì›Œìš” ë§¤ì¼ì…ë‹ˆë‹¤.\n",
      "   8. ì†”ì§íˆ ì‘ì€ ì¼ì—ë„ ê¹œì§ê¹œì§ ë†€ë¼ìš” ì •ë§ìš”.\n",
      "   9. ê·¸ëƒ¥ ë¶ˆì•ˆí•´ì„œ ì ì„ ì˜ ìˆ˜ê°€ ì—†ì–´ìš” ìê¾¸ìš”.\n",
      "  10. ì´ìƒí•˜ê²Œë„ ë‚´ì¼ ì‹œí—˜ ìƒê°ë§Œ í•´ë„ ê°€ìŠ´ì´ ë›°ì–´ìš” ë§¤ì¼.\n",
      "\n",
      "ë¶ˆì•ˆ ë¼ë²¨ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤:\n",
      "    ê°€ìŠ´ì´: 216íšŒ\n",
      "    ìê¾¸: 212íšŒ\n",
      "    ìš”ì¦˜: 138íšŒ\n",
      "    ë”°ë¼: 122íšŒ\n",
      "    ì†”ì§íˆ: 120íšŒ\n",
      "    ì‚¬ì‹¤ì€: 120íšŒ\n",
      "    ë‚˜ë§Œ: 117íšŒ\n",
      "    ë’¤ì²˜ì§ˆê¹Œ: 117íšŒ\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ­ ì •ìƒ ë¼ë²¨ ìƒ˜í”Œ (10ê°œ):\n",
      "   1. ì´ìƒí•˜ê²Œë„ ì¢‹ì•„í•˜ëŠ” ë…¸ë˜ë¥¼ ë“¤ìœ¼ë‹ˆ ê¸°ë¶„ì´ ì¢‹ì•„ì¡Œì–´ìš” ë³„ ì´ìœ  ì—†ì´ì‹¶ì–´ìš”.\n",
      "   2. ì†”ì§íˆ ì»¤í”¼ í•œ ì” ë§ˆì‹œë©´ì„œ ì—¬ìœ ë¥¼ ì¦ê²¼ì–´ìš”í•˜ë„¤ìš”.\n",
      "   3. ì†”ì§íˆ ë°¥ì´ ë„ˆë¬´ ë§›ìˆê²Œ ëŠê»´ì¡Œì–´ìš” ì •ë§ìš”.\n",
      "   4. ë¬¸ë“ ì»¤í”¼ í•œ ì” ë§ˆì‹œë©´ì„œ ì—¬ìœ ë¥¼ ì¦ê²¼ì–´ìš” ë§¤ì¼ë„¤ìš”.\n",
      "   5. ì†”ì§íˆ ì»¤í”¼ í•œ ì” ë§ˆì‹œë©´ì„œ ì—¬ìœ ë¥¼ ì¦ê²¼ì–´ìš” ë„ˆë¬´ìš”.\n",
      "   6. ê·¸ëƒ¥ ë°¥ì´ ë„ˆë¬´ ë§›ìˆê²Œ ëŠê»´ì¡Œì–´ìš”.\n",
      "   7. ê°€ì¡±ê³¼ í•¨ê»˜ ë³´ë‚¸ ì‹œê°„ì´ ì†Œì¤‘í–ˆì–´ìš” ë³„ ì´ìœ  ì—†ì´ìš”.\n",
      "   8. ë¬¸ë“ ê°€ì¡±ê³¼ í•¨ê»˜ ë³´ë‚¸ ì‹œê°„ì´ ì†Œì¤‘í–ˆì–´ìš” í•­ìƒì‹¶ì–´ìš”.\n",
      "   9. ë¬¸ë“ ì±… ì½ìœ¼ë©´ì„œ ì¡°ìš©í•œ ì‹œê°„ì„ ë³´ëƒˆì–´ìš” ìê¾¸ì‹¶ì–´ìš”.\n",
      "  10. ë¬¸ë“ ì£¼ë§ì— ë§›ì§‘ íƒë°©í•˜ëŸ¬ ë‹¤ë…€ì™”ì–´ìš” ìš”ì¦˜ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì •ìƒ ë¼ë²¨ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤:\n",
      "    ê¸°ë¶„ì´: 186íšŒ\n",
      "    ì¢‹ì•„ì¡Œì–´ìš”: 171íšŒ\n",
      "    ê·¸ëƒ¥: 127íšŒ\n",
      "    ì†”ì§íˆ: 126íšŒ\n",
      "    ìš”ì¦˜: 124íšŒ\n",
      "    ì •ë§: 118íšŒ\n",
      "    ì–´ëŠ: 117íšŒ\n",
      "    ìˆœê°„ë¶€í„°: 117íšŒ\n",
      "    ë”°ë¼: 116íšŒ\n",
      "    ë„ˆë¬´: 115íšŒ\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ê° ê°ì •ë³„ ì‹¤ì œ ë°ì´í„° ë‚´ìš© ë¶„ì„\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "print(\"ğŸ“ ê° ê°ì •ë³„ ì‹¤ì œ ë¬¸ì¥ ìƒ˜í”Œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "raw_df = pd.read_csv('../data/raw/korean_sentiment_dataset.csv')\n",
    "\n",
    "for label in [0, 1, 2]:\n",
    "    emotion = ['ìš°ìš¸', 'ë¶ˆì•ˆ', 'ì •ìƒ'][label]\n",
    "    emotion_data = raw_df[raw_df['label'] == label]\n",
    "    \n",
    "    print(f\"\\nğŸ­ {emotion} ë¼ë²¨ ìƒ˜í”Œ (10ê°œ):\")\n",
    "    samples = emotion_data['text'].head(10).tolist()\n",
    "    for i, text in enumerate(samples, 1):\n",
    "        print(f\"  {i:2d}. {text}\")\n",
    "    \n",
    "    print(f\"\\n{emotion} ë¼ë²¨ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤:\")\n",
    "    all_text = ' '.join(emotion_data['text'].astype(str))\n",
    "    # ê°„ë‹¨í•œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "    words = re.findall(r'[ê°€-í£]+', all_text)\n",
    "    word_freq = Counter(words)\n",
    "    top_words = word_freq.most_common(10)\n",
    "    for word, freq in top_words:\n",
    "        if len(word) > 1:  # í•œ ê¸€ì ë‹¨ì–´ ì œì™¸\n",
    "            print(f\"    {word}: {freq}íšŒ\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª ìƒˆë¡œìš´ ëª¨ë¸ ì‹¤ì „ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\n",
      "ì…ë ¥: 'ë°¥ì´ ë§›ìˆë‹¤'\n",
      "ì²˜ë¦¬: 'ë°¥ì´ ë§›ìˆë‹¤'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.824)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.081) ë¶ˆì•ˆ(0.095) ì •ìƒ(0.824)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ë‚ ì´ í™”ì°½í•´ìš”'\n",
      "ì²˜ë¦¬: 'ë‚ ì´ í™”ì°½í•´ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.352)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.322) ë¶ˆì•ˆ(0.326) ì •ìƒ(0.352)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•´ìš”'\n",
      "ì²˜ë¦¬: 'ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•´ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.816)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.091) ë¶ˆì•ˆ(0.093) ì •ìƒ(0.816)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ë„ˆë¬´ ë¶ˆì•ˆí•˜ê³  ê±±ì •ë¼ìš”'\n",
      "ì²˜ë¦¬: 'ë„ˆë¬´ ë¶ˆì•ˆí•˜ê³  ê±±ì •ë¼ìš”'\n",
      "ì˜ˆì¸¡: ìš°ìš¸ (0.833)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.833) ë¶ˆì•ˆ(0.073) ì •ìƒ(0.094)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ê¸°ë¶„ì´ ì •ë§ ì¢‹ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ê¸°ë¶„ì´ ì •ë§ ì¢‹ì•„ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.930)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.034) ë¶ˆì•ˆ(0.035) ì •ìƒ(0.930)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”'\n",
      "ì²˜ë¦¬: 'ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”'\n",
      "ì˜ˆì¸¡: ìš°ìš¸ (0.877)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.877) ë¶ˆì•ˆ(0.060) ì •ìƒ(0.062)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ì‹œí—˜ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ì‹œí—˜ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ìš”'\n",
      "ì˜ˆì¸¡: ë¶ˆì•ˆ (0.666)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.155) ë¶ˆì•ˆ(0.666) ì •ìƒ(0.180)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ê°€ì„œ ì¦ê±°ì› ì–´ìš”'\n",
      "ì²˜ë¦¬: 'ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ê°€ì„œ ì¦ê±°ì› ì–´ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.352)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.322) ë¶ˆì•ˆ(0.326) ì •ìƒ(0.352)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ê·¸ëƒ¥ ê·¸ë˜ìš”'\n",
      "ì²˜ë¦¬: 'ê·¸ëƒ¥ ê·¸ë˜ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.356)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.343) ë¶ˆì•ˆ(0.300) ì •ìƒ(0.356)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ë³„ë¡œ ì•ˆ ì¢‹ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ë³„ë¡œ ì¢‹ì•„ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.685)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.156) ë¶ˆì•ˆ(0.159) ì •ìƒ(0.685)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ê´œì°®ì€ ê²ƒ ê°™ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ê´œì°®ì€ ê°™ì•„ìš”'\n",
      "ì˜ˆì¸¡: ìš°ìš¸ (0.769)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.769) ë¶ˆì•ˆ(0.112) ì •ìƒ(0.119)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ê¸°ë¶„ì´ ì •ë§ ì¢‹ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ê¸°ë¶„ì´ ì •ë§ ì¢‹ì•„ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.930)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.034) ë¶ˆì•ˆ(0.035) ì •ìƒ(0.930)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”'\n",
      "ì²˜ë¦¬: 'ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”'\n",
      "ì˜ˆì¸¡: ìš°ìš¸ (0.877)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.877) ë¶ˆì•ˆ(0.060) ì •ìƒ(0.062)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ì‹œí—˜ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ì‹œí—˜ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ìš”'\n",
      "ì˜ˆì¸¡: ë¶ˆì•ˆ (0.666)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.155) ë¶ˆì•ˆ(0.666) ì •ìƒ(0.180)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ê°€ì„œ ì¦ê±°ì› ì–´ìš”'\n",
      "ì²˜ë¦¬: 'ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ê°€ì„œ ì¦ê±°ì› ì–´ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.352)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.322) ë¶ˆì•ˆ(0.326) ì •ìƒ(0.352)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ê·¸ëƒ¥ ê·¸ë˜ìš”'\n",
      "ì²˜ë¦¬: 'ê·¸ëƒ¥ ê·¸ë˜ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.356)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.343) ë¶ˆì•ˆ(0.300) ì •ìƒ(0.356)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ë³„ë¡œ ì•ˆ ì¢‹ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ë³„ë¡œ ì¢‹ì•„ìš”'\n",
      "ì˜ˆì¸¡: ì •ìƒ (0.685)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.156) ë¶ˆì•ˆ(0.159) ì •ìƒ(0.685)\n",
      "--------------------------------------------------\n",
      "ì…ë ¥: 'ê´œì°®ì€ ê²ƒ ê°™ì•„ìš”'\n",
      "ì²˜ë¦¬: 'ê´œì°®ì€ ê°™ì•„ìš”'\n",
      "ì˜ˆì¸¡: ìš°ìš¸ (0.769)\n",
      "ì „ì²´ í™•ë¥ : ìš°ìš¸(0.769) ë¶ˆì•ˆ(0.112) ì •ìƒ(0.119)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª ìƒˆë¡œìš´ ëª¨ë¸ ì‹¤ì „ í…ŒìŠ¤íŠ¸ - ë¬¸ì œ ë¬¸ì¥ë“¤ í¬í•¨\n",
    "print(\"ğŸ§ª ìƒˆë¡œìš´ ëª¨ë¸ ì‹¤ì „ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_new_model(text):\n",
    "    # ì „ì²˜ë¦¬ (ì›¹ì•±ê³¼ ë™ì¼)\n",
    "    cleaned = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', str(text))\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    # ë¶ˆìš©ì–´ ì œê±°\n",
    "    STOPWORDS = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'í•˜ë‹¤']\n",
    "    words = cleaned.split()\n",
    "    processed = ' '.join([word for word in words if word not in STOPWORDS and len(word) > 1])\n",
    "    \n",
    "    if not processed.strip():\n",
    "        processed = cleaned  # ë¶ˆìš©ì–´ ì œê±° í›„ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì›ë³¸ ì‚¬ìš©\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    text_tfidf = tfidf.transform([processed])\n",
    "    prediction = best_model.predict(text_tfidf)[0]\n",
    "    probability = best_model.predict_proba(text_tfidf)[0]\n",
    "    \n",
    "    emotion_map = {0: 'ìš°ìš¸', 1: 'ë¶ˆì•ˆ', 2: 'ì •ìƒ'}\n",
    "    \n",
    "    print(f\"ì…ë ¥: '{text}'\")\n",
    "    print(f\"ì²˜ë¦¬: '{processed}'\")\n",
    "    print(f\"ì˜ˆì¸¡: {emotion_map[prediction]} ({probability[prediction]:.3f})\")\n",
    "    print(f\"ì „ì²´ í™•ë¥ : ìš°ìš¸({probability[0]:.3f}) ë¶ˆì•ˆ({probability[1]:.3f}) ì •ìƒ({probability[2]:.3f})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return emotion_map[prediction]\n",
    "\n",
    "# ë¬¸ì œê°€ ìˆë˜ ë¬¸ì¥ë“¤ê³¼ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
    "test_sentences = [\n",
    "    # ì‚¬ìš©ìê°€ ë¬¸ì œ ì œê¸°í•œ ë¬¸ì¥\n",
    "    \"ë°¥ì´ ë§›ìˆë‹¤\",\n",
    "    \"ë‚ ì´ í™”ì°½í•´ìš”\",\n",
    "    \n",
    "    # ëª…í™•í•œ ê°ì • í‘œí˜„ë“¤\n",
    "    \"ì˜¤ëŠ˜ ì •ë§ ìš°ìš¸í•´ìš”\",\n",
    "    \"ë„ˆë¬´ ë¶ˆì•ˆí•˜ê³  ê±±ì •ë¼ìš”\",\n",
    "    \"ê¸°ë¶„ì´ ì •ë§ ì¢‹ì•„ìš”\",\n",
    "    \n",
    "    # ê°„ì ‘ì  í‘œí˜„ë“¤\n",
    "    \"ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”\",\n",
    "    \"ì‹œí—˜ ë•Œë¬¸ì— ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ìš”\",\n",
    "    \"ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ê°€ì„œ ì¦ê±°ì› ì–´ìš”\",\n",
    "    \n",
    "    # ì• ë§¤í•œ í‘œí˜„ë“¤\n",
    "    \"ê·¸ëƒ¥ ê·¸ë˜ìš”\",\n",
    "    \"ë³„ë¡œ ì•ˆ ì¢‹ì•„ìš”\",\n",
    "    \"ê´œì°®ì€ ê²ƒ ê°™ì•„ìš”\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
    "for sentence in test_sentences:\n",
    "    test_new_model(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
